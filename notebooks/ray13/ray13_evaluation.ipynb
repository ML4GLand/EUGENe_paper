{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85c10a31",
   "metadata": {},
   "source": [
    "# Ray et al 2013 Evaluation\n",
    "**Authorship:**\n",
    "Adam Klie, *08/31/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook to evaluate trained models on the Ray et al (2013) dataset. Evaulating across all k-mers is a very, very slow process. Please check out the accompanying `ray13_evaluation_{setA|ST|MT|Kipoi}.py` scripts, along with the SLURM `ray13_evaluation.sh` script for how to run this on a compute cluster over multiple days.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a935404",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eugene as eu\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ffafd6",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "eu.settings.dataset_dir = \"/cellar/users/aklie/data/eugene/ray13\"\n",
    "eu.settings.output_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/output/ray13\"\n",
    "eu.settings.logging_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/logs/ray13\"\n",
    "eu.settings.config_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/configs/ray13\"\n",
    "eu.settings.figure_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/figures/ray13\"\n",
    "eu.settings.verbosity = logging.ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e853e3",
   "metadata": {},
   "source": [
    "# Load in the test `SeqData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf28384-5f24-4a86-8b11-ba91089f7005",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "sdata_test = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, \"norm_setB_processed_ST.h5sd\"))\n",
    "target_mask = sdata_test.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols = sdata_test.seqs_annot.columns[target_mask]\n",
    "sdata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e39c6",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the Set B presence/absence predictions\n",
    "b_presence_absence = np.load(os.path.join(eu.settings.dataset_dir, \"setB_binary.npy\"))\n",
    "setB_observed = sdata_test.seqs_annot[target_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd90ca6",
   "metadata": {},
   "source": [
    "# Get test set evaluations for each model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fda7c37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SetA k-mer scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b2659",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the Set A presence/absence predictions\n",
    "a_presence_absence = np.load(os.path.join(eu.settings.dataset_dir, \"SetA_binary_ST.npy\"))\n",
    "setA_observed = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, eu.settings.dataset_dir, \"norm_setA_processed_ST.h5sd\")).seqs_annot[target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b6dec",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# An example calculation of z-scores, aucs and e-scores for Set A and Set B\n",
    "a_metr = eu.predict.rna_complete_metrics_apply(a_presence_absence, setA_observed[\"RNCMPT00001\"].values, verbose=True, num_kmers=100)\n",
    "b_metr = eu.predict.rna_complete_metrics_apply(b_presence_absence, setB_observed[\"RNCMPT00001\"].values, verbose=True, num_kmers=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e67845",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Performing the above calculation for all targets (TODO: parallelize and simplify)\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "pearson_setA_long = pd.DataFrame()\n",
    "spearman_setA_long = pd.DataFrame()\n",
    "for i, task in tqdm(enumerate(target_cols[:3]), desc=\"Calcualting metrics on each task\", total=len(target_cols)):\n",
    "    a_zscores, a_aucs, a_escores  = eu.predict.rna_complete_metrics_apply(a_presence_absence, setA_observed[task].values, verbose=True, use_calc_auc=True, num_kmers=100)\n",
    "    b_zscores, b_aucs, b_escores = eu.predict.rna_complete_metrics_apply(b_presence_absence, setB_observed[task].values, verbose=True, use_calc_auc=True, num_kmers=100) \n",
    "    \n",
    "    zscore_nan_mask = np.isnan(a_zscores) | np.isnan(b_zscores)\n",
    "    a_zscores = a_zscores[~zscore_nan_mask]\n",
    "    b_zscores = b_zscores[~zscore_nan_mask]\n",
    "    if len(a_zscores) > 0 and len(b_zscores) > 0:\n",
    "        pearson_setA_long = pearson_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"Z-score\", \"Pearson\": pearsonr(a_zscores, b_zscores)[0]}), ignore_index=True)\n",
    "        spearman_setA_long = spearman_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"Z-score\", \"Spearman\": spearmanr(a_zscores, b_zscores)[0]}), ignore_index=True)\n",
    "\n",
    "    auc_nan_mask = np.isnan(a_aucs) | np.isnan(b_aucs)\n",
    "    a_aucs = a_aucs[~auc_nan_mask]\n",
    "    b_aucs = b_aucs[~auc_nan_mask]\n",
    "    if len(a_aucs) > 0 and len(b_aucs) > 0:\n",
    "        pearson_setA_long = pearson_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"AUC\", \"Pearson\": pearsonr(a_aucs, b_aucs)[0]}), ignore_index=True)\n",
    "        spearman_setA_long = spearman_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"AUC\", \"Spearman\": spearmanr(a_aucs, b_aucs)[0]}), ignore_index=True)\n",
    "    \n",
    "    escore_nan_mask = np.isnan(a_escores) | np.isnan(b_escores)\n",
    "    a_escores = a_escores[~escore_nan_mask]\n",
    "    b_escores = b_escores[~escore_nan_mask]\n",
    "    if len(a_escores) > 0 and len(b_escores) > 0:\n",
    "        pearson_setA_long = pearson_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"E-score\", \"Pearson\": pearsonr(a_escores, b_escores)[0]}), ignore_index=True)\n",
    "        spearman_setA_long = spearman_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"E-score\", \"Spearman\": spearmanr(a_escores, b_escores)[0]}), ignore_index=True)\n",
    "\n",
    "pearson_setA_long[\"Model\"] = \"SetA\"\n",
    "spearman_setA_long[\"Model\"] = \"SetA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c20e7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "del a_presence_absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae167d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Plot just the SetA results \n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_setA_long, x=\"Metric\", y=\"Pearson\", color=\"green\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_setA_long, x=\"Metric\", y=\"Spearman\", color=\"green\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figure_dir, \"correlation_boxplots_setA.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10790af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Single-task model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54475a73",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on the test data from all single task models\n",
    "for i, target_col in enumerate(target_cols[:20]):\n",
    "    print(f\"Testing DeepBind SingleTask model on {target_col}\")\n",
    "    try:\n",
    "        model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_ST\", target_col, \"checkpoints\", \"*\"))[0]\n",
    "        model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "    except:\n",
    "        print(f\"No model trained for {target_col}\")\n",
    "        continue\n",
    "    eu.settings.dl_num_workers = 0\n",
    "    eu.predict.predictions(\n",
    "        model,\n",
    "        sdata=sdata_test, \n",
    "        target=target_col,\n",
    "        name=\"DeepBind_ST\",\n",
    "        version=target_col,\n",
    "        file_label=\"test\",\n",
    "        suffix=\"_ST\"\n",
    "    )\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891dd20",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get evaluation metrics for all single task models and format for plotting\n",
    "pearson_ST_df, spearman_ST_df = eu.predict.summarize_rbps_apply(sdata_test, b_presence_absence, target_cols[:3], use_calc_auc=True, verbose=True, n_kmers=100, preds_suffix=\"_predictions_ST\")\n",
    "pearson_ST_long = pearson_ST_df.reset_index().melt(id_vars=\"index\", value_name=\"Pearson\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "spearman_ST_long = spearman_ST_df.reset_index().melt(id_vars=\"index\", value_name=\"Spearman\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "pearson_ST_long[\"Model\"] = \"SingleTask\"\n",
    "spearman_ST_long[\"Model\"] = \"SingleTask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366f7ee",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Plot just the single task model eval\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_ST_long, x=\"Metric\", y=\"Pearson\", color=\"red\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_ST_long, x=\"Metric\", y=\"Spearman\", color=\"red\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figure_dir, \"correlation_boxplots_ST.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f3fdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multitask model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902efa40",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Also need the multi-task columns (single task we could train on all the columns)\n",
    "sdata_training = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, eu.settings.dataset_dir, \"norm_setA_sub_MT.h5sd\"))\n",
    "target_mask_MT = sdata_training.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols_MT = sdata_training.seqs_annot.columns[target_mask_MT]\n",
    "del sdata_training\n",
    "len(target_cols_MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9c79f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions on the test data from all multi task models\n",
    "print(\"Testing DeepBind MultiTask model on\")\n",
    "version = 0\n",
    "model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_MT\", f\"v{version}\", \"checkpoints\", \"*\"))[0]\n",
    "model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "eu.settings.dl_num_workers = 0\n",
    "eu.predict.predictions(\n",
    "    model,\n",
    "    sdata=sdata_test, \n",
    "    target=target_cols_MT,\n",
    "    name=\"DeepBind_MT\",\n",
    "    version=f\"v{version}\",\n",
    "    file_label=\"test\",\n",
    "    suffix=\"_MT\"\n",
    ")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3241bcd",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get evaluation metrics for all single task models and format for plotting\n",
    "pearson_MT_df, spearman_MT_df = eu.predict.summarize_rbps_apply(sdata_test, b_presence_absence, target_cols_MT[:3], use_calc_auc=True, verbose=True, n_kmers=100, preds_suffix=\"_predictions_MT\")\n",
    "pearson_MT_long = pearson_MT_df.reset_index().melt(id_vars=\"index\", value_name=\"Pearson\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "spearman_MT_long = spearman_MT_df.reset_index().melt(id_vars=\"index\", value_name=\"Spearman\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "pearson_MT_long[\"Model\"] = \"MultiTask\"\n",
    "spearman_MT_long[\"Model\"] = \"MultiTask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7716b42",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Plot just the multi task model eval\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_MT_long, x=\"Metric\", y=\"Pearson\", color=\"blue\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_MT_long, x=\"Metric\", y=\"Spearman\", color=\"blue\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figure_dir, \"correlation_boxplots_MT.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875251c4-4028-4039-a22d-bbf35a208ed8",
   "metadata": {},
   "source": [
    "## Kipoi models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984ad0c-728c-44b4-adc7-32097b51dda4",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We need to get the protein IDs from the motifs in the\n",
    "id_mapping = pd.read_excel(os.path.join(eu.settings.dataset_dir, \"hg19_motif_hits\", \"ID.mapping.xls\"), sheet_name=0)\n",
    "id_mp = id_mapping.set_index(\"Motif ID\")[\"Protein(s)\"]\n",
    "cols_w_ids = ~target_cols.map(id_mp).isna()\n",
    "target_cols_w_ids = target_cols[cols_w_ids]\n",
    "ids_w_target_cols = pd.Index([id.split(\"(\")[0].rstrip() for id in target_cols_w_ids.map(id_mp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10071e7-25ea-4596-8af5-a6fe4b82529f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get the kipoi models names\n",
    "db_model_names = eu.external.kipoi.get_model_names(\"DeepBind/Homo_sapiens/RBP/D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0904f8b-0161-4d02-a26e-d86713d48b59",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get predictions with each model and store them in sdata\n",
    "target_cols_w_model = []\n",
    "for i, (protein_id , motif_id) in tqdm(enumerate(zip(ids_w_target_cols, target_cols_w_ids)), desc=\"Importing models\", total=len(ids_w_target_cols)):\n",
    "    print(\"Predicting for protein: \", protein_id, \" motif: \", motif_id)\n",
    "    db_model_name = db_model_names[db_model_names.str.contains(protein_id)]\n",
    "    if len(db_model_name) == 0:\n",
    "        print(\"No model found for protein: \", protein_id)\n",
    "        continue\n",
    "    try:\n",
    "        model = eu.external.kipoi.get_model(db_model_name.values[0])\n",
    "        sdata_test[f\"{motif_id}_predictions_kipoi\"] = model(sdata_test.ohe_seqs.transpose(0,2,1)).cpu().numpy()\n",
    "        target_cols_w_model.append(motif_id)\n",
    "    except:\n",
    "        print(\"Failed to load model\")\n",
    "    if len(target_cols_w_model) == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622b2e55-e2d1-4aa3-9cc0-238af29dfe50",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Save the sdata with kipoi predictions\n",
    "sdata_test.write_h5sd(os.path.join(eu.settings.output_dir, \"norm_test_predictions.h5sd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af70cb-2f80-4d12-95dd-3a579f7b5d56",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate the predictions using the RNAcompete metrics\n",
    "pearson_kipoi_df, spearman_kipoi_df = eu.predict.rnacomplete_metrics_sdata_table(sdata_test, b_presence_absence, target_cols_w_model, verbose=False, num_kmers=5, preds_suffix=\"_predictions_kipoi\")\n",
    "pearson_kipoi_long = pearson_kipoi_df.reset_index().melt(id_vars=\"index\", value_name=\"Pearson\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "spearman_kipoi_long = spearman_kipoi_df.reset_index().melt(id_vars=\"index\", value_name=\"Spearman\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "pearson_kipoi_long[\"Model\"] = \"Kipoi\"\n",
    "spearman_kipoi_long[\"Model\"] = \"Kipoi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00328da-dce8-447e-8b9d-5ac8d55d4584",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Plot just the kipoi results as boxplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_kipoi_long, x=\"Metric\", y=\"Pearson\", color=\"orange\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_kipoi_long, x=\"Metric\", y=\"Spearman\", color=\"orange\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(figure_dir, \"correlation_boxplots_kipoi_10kmers_.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fd5c3-6396-4a37-943d-efadbe8fb881",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "pearson_kipoi_long.to_csv(os.path.join(eu.settings.output_dir, f\"pearson_performance_{number_kmers}kmers_kipoi.tsv\"), index=False, sep=\"\\t\")\n",
    "spearman_kipoi_long.to_csv(os.path.join(eu.settings.output_dir, f\"spearman_performance_{number_kmers}kmers_kipoi.tsv\"), index=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d95b1-b397-49e2-87a2-084f63797885",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 eugene_dev",
   "language": "python",
   "name": "eugene_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
