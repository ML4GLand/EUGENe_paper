{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85c10a31",
   "metadata": {},
   "source": [
    "# Ray et al 2013 Evaluation\n",
    "**Authorship:**\n",
    "Adam Klie (last updated: *06/09/2023*)\n",
    "***\n",
    "**Description:**\n",
    "Notebook to evaluate trained models on the Ray et al (2013) dataset. Evaulating across all k-mers is a very, very slow process. Please check out the accompanying `ray13_evaluation_{setA|ST|MT|Kipoi}.py` scripts, along with the SLURM `ray13_evaluation.sh` script for how to run this on a compute cluster over multiple days.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5feacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# EUGENe imports and settings\n",
    "import eugene as eu\n",
    "from eugene import models, train, evaluate, settings\n",
    "from eugene.models import zoo\n",
    "settings.dataset_dir = \"/cellar/users/aklie/data/eugene/revision/ray13\"\n",
    "settings.output_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/output/revision/ray13\"\n",
    "settings.logging_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/logs/revision/ray13\"\n",
    "settings.config_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/configs/ray13\"\n",
    "settings.figure_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/figures/revision/ray13\"\n",
    "\n",
    "# EUGENe packages\n",
    "import seqdata as sd\n",
    "\n",
    "# ray13 helpers\n",
    "sys.path.append(\"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/scripts/ray13\")\n",
    "from ray13_helpers import rnacomplete_metrics, rnacomplete_metrics_sdata_table\n",
    "\n",
    "# Print versions\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Eugene version: {eu.__version__}\")\n",
    "print(f\"SeqData version: {sd.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pytorch_lightning.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23463df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "sdata_test = sd.open_zarr(os.path.join(settings.dataset_dir, \"norm_setB_ST.zarr\"))\n",
    "keys = pd.Index(sdata_test.data_vars.keys())\n",
    "target_mask = keys.str.contains(\"RNCMPT\")\n",
    "target_cols = keys[target_mask]\n",
    "\n",
    "# Load in the Set B presence/absence predictions\n",
    "b_presence_absence = np.load(os.path.join(settings.dataset_dir, \"setB_binary.npy\"))\n",
    "setB_observed = sdata_test[target_cols]\n",
    "\n",
    "# Also need the multi-task columns (single task we could train on all the columns)\n",
    "sdata_training = sd.open_zarr(os.path.join(eu.settings.dataset_dir, \"norm_setA_sub_MT.zarr\"))\n",
    "keys_MT = pd.Index(sdata_training.data_vars.keys())\n",
    "target_mask_MT = keys_MT.str.contains(\"RNCMPT\")\n",
    "target_cols_MT = keys_MT[target_mask_MT]\n",
    "del sdata_training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21f11a69",
   "metadata": {},
   "source": [
    "# Number of kmers to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ae40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is used for testing and this number will define a small subset of kmers to use for that purpose\n",
    "number_kmers = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e853e3",
   "metadata": {},
   "source": [
    "# Load in the test `SeqData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf28384-5f24-4a86-8b11-ba91089f7005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "sdata_test = sd.open_zarr(os.path.join(settings.dataset_dir, \"norm_setB_ST.zarr\"))\n",
    "keys = pd.Index(sdata_test.data_vars.keys())\n",
    "target_mask = keys.str.contains(\"RNCMPT\")\n",
    "target_cols = keys[target_mask]\n",
    "sdata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e39c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the Set B presence/absence predictions\n",
    "b_presence_absence = np.load(os.path.join(settings.dataset_dir, \"setB_binary.npy\"))\n",
    "setB_observed = sdata_test[target_cols]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dd90ca6",
   "metadata": {},
   "source": [
    "# Get test set evaluations for each model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fda7c37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## SetA k-mer scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Set A presence/absence predictions\n",
    "a_presence_absence = np.load(os.path.join(settings.dataset_dir, \"setA_binary_ST.npy\"))\n",
    "setA_observed = sd.open_zarr(os.path.join(settings.dataset_dir, \"norm_setA_ST.zarr\"))[target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f228d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we have a good list of kmers\n",
    "if number_kmers is not None:\n",
    "    random_kmers = np.random.choice(np.arange(a_presence_absence.shape[0]), size=number_kmers)\n",
    "    a_presence_absence = a_presence_absence[random_kmers, :]\n",
    "    b_presence_absence = b_presence_absence[random_kmers, :]\n",
    "valid_kmers = np.where((np.sum(a_presence_absence, axis=1) > 0) & (np.sum(b_presence_absence, axis=1) > 0))[0]\n",
    "a_presence_absence = a_presence_absence[valid_kmers, :]\n",
    "b_presence_absence = b_presence_absence[valid_kmers, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265b6dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example calculation of z-scores, aucs and e-scores for Set A and Set B\n",
    "a_metr = rnacomplete_metrics(a_presence_absence, setA_observed[\"RNCMPT00001\"].values, verbose=True)\n",
    "b_metr = rnacomplete_metrics(b_presence_absence, setB_observed[\"RNCMPT00001\"].values, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e67845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performing the above calculation for all targets (TODO: parallelize and simplify)\n",
    "pearson_setA_long = pd.DataFrame()\n",
    "spearman_setA_long = pd.DataFrame()\n",
    "for i, task in tqdm(enumerate(target_cols[:10]), desc=\"Calculating metrics on each task\", total=len(target_cols)):\n",
    "    a_zscores, a_aucs, a_escores  = rnacomplete_metrics(a_presence_absence, setA_observed[task].values, verbose=False)\n",
    "    b_zscores, b_aucs, b_escores = rnacomplete_metrics(b_presence_absence, setB_observed[task].values, verbose=False)\n",
    "    try:\n",
    "        zscore_nan_mask = np.isnan(a_zscores) | np.isnan(b_zscores)\n",
    "        a_zscores = a_zscores[~zscore_nan_mask]\n",
    "        b_zscores = b_zscores[~zscore_nan_mask]\n",
    "        if len(a_zscores) > 0 and len(b_zscores) > 0:\n",
    "            pearson_setA_long = pearson_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"Z-score\", \"Pearson\": pearsonr(a_zscores, b_zscores)[0]}), ignore_index=True)\n",
    "            spearman_setA_long = spearman_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"Z-score\", \"Spearman\": spearmanr(a_zscores, b_zscores)[0]}), ignore_index=True)\n",
    "\n",
    "        auc_nan_mask = np.isnan(a_aucs) | np.isnan(b_aucs)\n",
    "        a_aucs = a_aucs[~auc_nan_mask]\n",
    "        b_aucs = b_aucs[~auc_nan_mask]\n",
    "        if len(a_aucs) > 0 and len(b_aucs) > 0:\n",
    "            pearson_setA_long = pearson_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"AUC\", \"Pearson\": pearsonr(a_aucs, b_aucs)[0]}), ignore_index=True)\n",
    "            spearman_setA_long = spearman_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"AUC\", \"Spearman\": spearmanr(a_aucs, b_aucs)[0]}), ignore_index=True)\n",
    "\n",
    "        escore_nan_mask = np.isnan(a_escores) | np.isnan(b_escores)\n",
    "        a_escores = a_escores[~escore_nan_mask]\n",
    "        b_escores = b_escores[~escore_nan_mask]\n",
    "        if len(a_escores) > 0 and len(b_escores) > 0:\n",
    "            pearson_setA_long = pearson_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"E-score\", \"Pearson\": pearsonr(a_escores, b_escores)[0]}), ignore_index=True)\n",
    "            spearman_setA_long = spearman_setA_long.append(pd.Series({\"RBP\": task, \"Metric\": \"E-score\", \"Spearman\": spearmanr(a_escores, b_escores)[0]}), ignore_index=True)\n",
    "    \n",
    "    except:\n",
    "        print(f\"Could not evaluate {task}, skipping\")\n",
    "        continue\n",
    "pearson_setA_long[\"Model\"] = \"SetA\"\n",
    "spearman_setA_long[\"Model\"] = \"SetA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c20e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "del a_presence_absence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot just the SetA results \n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_setA_long, x=\"Metric\", y=\"Pearson\", color=\"green\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_setA_long, x=\"Metric\", y=\"Spearman\", color=\"green\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(settings.figure_dir, \"correlation_boxplots_setA.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c10790af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Single-task model evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54475a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the DeepBind architecture\n",
    "arch = zoo.DeepBind(\n",
    "    input_len=41,\n",
    "    output_dim=1,\n",
    "    conv_kwargs=dict(input_channels=4, conv_channels=[16], conv_kernels=[16], dropout_rates=0.5, batchnorm=True),\n",
    "    dense_kwargs=dict(hidden_dims=[32], dropout_rates=0.5, batchnorm=True),\n",
    ")\n",
    "\n",
    "# Get predictions from each single task model, keeping track of which we could successfully get predictions from\n",
    "trained_model_cols = []\n",
    "for i, target_col in enumerate(target_cols[:10]):\n",
    "    print(f\"Testing DeepBind SingleTask model on {target_col}\")\n",
    "    try:\n",
    "        model_file = glob.glob(os.path.join(settings.logging_dir, \"DeepBind_ST\", target_col, \"checkpoints\", \"*\"))[0]\n",
    "        model = models.SequenceModule.load_from_checkpoint(model_file, arch=arch)\n",
    "        trained_model_cols.append(target_col)\n",
    "    except:\n",
    "        print(f\"No model trained for {target_col}\")\n",
    "        continue\n",
    "    evaluate.predictions_sequence_module(\n",
    "        model,\n",
    "        sdata=sdata_test, \n",
    "        seq_key=\"ohe_seq\",\n",
    "        target_keys=target_col,\n",
    "        batch_size=5096,\n",
    "        num_workers=4,\n",
    "        prefetch_factor=2,\n",
    "        in_memory=True,\n",
    "        transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32), \"target\": lambda x: torch.tensor(x, dtype=torch.float32)},\n",
    "        name=\"DeepBind_ST\",\n",
    "        version=target_col,\n",
    "        file_label=\"test\",\n",
    "        suffix=\"_ST\"\n",
    "    )\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891dd20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get evaluation metrics for all single task models and format for plotting\n",
    "pearson_ST_df, spearman_ST_df = rnacomplete_metrics_sdata_table(sdata_test, b_presence_absence, trained_model_cols, verbose=False, swifter=True, num_kmers=10, preds_suffix=\"_predictions_ST\")\n",
    "pearson_ST_long = pearson_ST_df.reset_index().melt(id_vars=\"index\", value_name=\"Pearson\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "spearman_ST_long = spearman_ST_df.reset_index().melt(id_vars=\"index\", value_name=\"Spearman\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "pearson_ST_long[\"Model\"] = \"SingleTask\"\n",
    "spearman_ST_long[\"Model\"] = \"SingleTask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot just the single task model eval\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_ST_long, x=\"Metric\", y=\"Pearson\", color=\"red\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_ST_long, x=\"Metric\", y=\"Spearman\", color=\"red\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(settings.figure_dir, \"correlation_boxplots_ST.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69f3fdc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Multitask model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902efa40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Also need the multi-task columns (single task we could train on all the columns)\n",
    "sdata_training = sd.open_zarr(os.path.join(settings.dataset_dir, \"norm_setA_sub_MT.zarr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3554197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also need the multi-task columns (single task we could train on all the columns)\n",
    "keys_MT = pd.Index(sdata_training.data_vars.keys())\n",
    "target_mask_MT = keys_MT.str.contains(\"RNCMPT\")\n",
    "target_cols_MT = keys_MT[target_mask_MT]\n",
    "del sdata_training\n",
    "len(target_cols_MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9c79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the DeepBind multitask model of choice (one of the versions)\n",
    "print(\"Testing DeepBind MultiTask model on\")\n",
    "version = 0\n",
    "arch = models.zoo.DeepBind(\n",
    "    input_len=41, # Length of padded sequences\n",
    "    output_dim=len(target_cols_MT), # Number of multitask outputs\n",
    "    conv_kwargs=dict(input_channels=4, conv_channels=[1024], conv_kernels=[16], dropout_rates=0.25, batchnorm=0.25),\n",
    "    dense_kwargs=dict(hidden_dims=[512], dropout_rates=0.25, batchnorm=True),\n",
    ")\n",
    "model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_MT\", f\"v{version}\", \"checkpoints\", \"*\"))[0]\n",
    "model = models.SequenceModule.load_from_checkpoint(model_file, arch=arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test data from the multi-task model\n",
    "evaluate.predictions_sequence_module(\n",
    "    model,\n",
    "    sdata=sdata_test,\n",
    "    seq_key=\"ohe_seq\",\n",
    "    target_keys=target_cols_MT,\n",
    "    batch_size=1024,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    in_memory=True,\n",
    "    transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32), \"target\": lambda x: torch.tensor(x, dtype=torch.float32)},\n",
    "    name=\"DeepBind_MT\",\n",
    "    version=f\"v{version}\",\n",
    "    file_label=\"test\",\n",
    "    suffix=\"_MT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3241bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get evaluation metrics for all single task models and format for plotting\n",
    "pearson_MT_df, spearman_MT_df = rnacomplete_metrics_sdata_table(sdata_test, b_presence_absence, target_cols_MT[:10], verbose=False, swifter=True, num_kmers=10, preds_suffix=\"_predictions_MT\")\n",
    "pearson_MT_long = pearson_MT_df.reset_index().melt(id_vars=\"index\", value_name=\"Pearson\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "spearman_MT_long = spearman_MT_df.reset_index().melt(id_vars=\"index\", value_name=\"Spearman\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "pearson_MT_long[\"Model\"] = \"MultiTask\"\n",
    "spearman_MT_long[\"Model\"] = \"MultiTask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7716b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot just the multi task model eval\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_MT_long, x=\"Metric\", y=\"Pearson\", color=\"blue\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_MT_long, x=\"Metric\", y=\"Spearman\", color=\"blue\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(settings.figure_dir, \"correlation_boxplots_MT.pdf\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "875251c4-4028-4039-a22d-bbf35a208ed8",
   "metadata": {},
   "source": [
    "## Kipoi models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8496f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to figure out how to handle this for revisions\n",
    "sys.path.append(\"/cellar/users/aklie/projects/ML4GLand/external/\")\n",
    "from kipoi_ext import get_model_names, get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4955862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "sdata_test = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, \"norm_setB_processed_ST.h5sd\"))\n",
    "target_mask = sdata_test.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols = sdata_test.seqs_annot.columns[target_mask]\n",
    "sdata_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984ad0c-728c-44b4-adc7-32097b51dda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to get the protein IDs from the motifs in the\n",
    "id_mapping = pd.read_excel(os.path.join(settings.dataset_dir, \"hg19_motif_hits\", \"ID.mapping.xls\"), sheet_name=0)\n",
    "id_mp = id_mapping.set_index(\"Motif ID\")[\"Protein(s)\"]\n",
    "cols_w_ids = ~target_cols.map(id_mp).isna()\n",
    "target_cols_w_ids = target_cols[cols_w_ids]\n",
    "ids_w_target_cols = pd.Index([id.split(\"(\")[0].rstrip() for id in target_cols_w_ids.map(id_mp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10071e7-25ea-4596-8af5-a6fe4b82529f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the kipoi models names\n",
    "db_model_names = get_model_names(\"DeepBind/Homo_sapiens/RBP/D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with each model and store them in sdata\n",
    "target_cols_w_model = []\n",
    "for i, (protein_id , motif_id) in tqdm(enumerate(zip(ids_w_target_cols, target_cols_w_ids)), desc=\"Importing models\", total=len(ids_w_target_cols)):\n",
    "    print(\"Predicting for protein: \", protein_id, \" motif: \", motif_id)\n",
    "    db_model_name = db_model_names[db_model_names.str.contains(protein_id)]\n",
    "    if len(db_model_name) == 0:\n",
    "        print(\"No model found for protein: \", protein_id)\n",
    "        continue\n",
    "    try:\n",
    "        model = get_model(db_model_name.values[0])\n",
    "        sdata_test[f\"{motif_id}_predictions_kipoi\"] = model(sdata_test[\"ohe_seq\"].values.transpose(0,2,1)).cpu().numpy()\n",
    "        target_cols_w_model.append(motif_id)\n",
    "    except:\n",
    "        print(\"Failed to load model\")\n",
    "    if len(target_cols_w_model) == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4af70cb-2f80-4d12-95dd-3a579f7b5d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the predictions using the RNAcompete metrics\n",
    "pearson_kipoi_df, spearman_kipoi_df = eu.predict.rnacomplete_metrics_sdata_table(sdata_test, b_presence_absence, target_cols_w_model, verbose=False, num_kmers=5, preds_suffix=\"_predictions_kipoi\")\n",
    "pearson_kipoi_long = pearson_kipoi_df.reset_index().melt(id_vars=\"index\", value_name=\"Pearson\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "spearman_kipoi_long = spearman_kipoi_df.reset_index().melt(id_vars=\"index\", value_name=\"Spearman\", var_name=\"Metric\").rename({\"index\":\"RBP\"}, axis=1)\n",
    "pearson_kipoi_long[\"Model\"] = \"Kipoi\"\n",
    "spearman_kipoi_long[\"Model\"] = \"Kipoi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00328da-dce8-447e-8b9d-5ac8d55d4584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot just the kipoi results as boxplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "sns.boxplot(data=pearson_kipoi_long, x=\"Metric\", y=\"Pearson\", color=\"orange\", ax=ax[0])\n",
    "sns.boxplot(data=spearman_kipoi_long, x=\"Metric\", y=\"Spearman\", color=\"orange\", ax=ax[1])\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(settings.figure_dir, \"correlation_boxplots_kipoi_10kmers_.pdf\"))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb99e8d9",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90e63da1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bbbfac7",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
