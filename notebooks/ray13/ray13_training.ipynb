{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray et al 2013 Training \n",
    "**Authorship:**\n",
    "Adam Klie, *08/31/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook to perform simple training of *single task* and *multitask* models on the Ray et al (2013) dataset.\n",
    "Also take a look at the `ray13_training_ST.py` script for usage. The script was run because all 244 models took several hours to train.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eugene as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eu.settings.dataset_dir = \"/cellar/users/aklie/data/eugene/ray13\"\n",
    "eu.settings.output_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/output/ray13\"\n",
    "eu.settings.logging_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/logs/ray13\"\n",
    "eu.settings.config_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/configs/ray13\"\n",
    "eu.settings.verbosity = logging.ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in the SetA training `SeqData`'s for single task and multi-task models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the training SetA processed data for single task and multitask models\n",
    "sdata_training_ST = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, eu.settings.dataset_dir, \"norm_setA_processed_ST.h5sd\"))\n",
    "sdata_training_MT = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, eu.settings.dataset_dir, \"norm_setA_processed_MT.h5sd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata_training_ST, sdata_training_MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the prediction columns for single task and multitask\n",
    "target_mask_ST = sdata_training_ST.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols_ST = sdata_training_ST.seqs_annot.columns[target_mask_ST]\n",
    "target_mask_MT = sdata_training_MT.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols_MT = sdata_training_MT.seqs_annot.columns[target_mask_MT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols_MT[215]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train single task models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation function\n",
    "from pytorch_lightning import seed_everything\n",
    "def prep_new_model(\n",
    "    seed,\n",
    "    conv_dropout = 0,\n",
    "    fc_dropout = 0,\n",
    "    batchnorm = True\n",
    "):\n",
    "    model = eu.models.DeepBind(\n",
    "        input_len=41, # Length of padded sequences\n",
    "        output_dim=1, # Number of multitask outputs\n",
    "        strand=\"ss\",\n",
    "        task=\"regression\",\n",
    "        conv_kwargs=dict(channels=[4, 16], conv_kernels=[16], dropout_rates=conv_dropout, batchnorm=batchnorm),\n",
    "        mp_kwargs=dict(kernel_size=8),\n",
    "        fc_kwargs=dict(hidden_dims=[32], dropout_rate=fc_dropout, batchnorm=batchnorm),\n",
    "        optimizer=\"sgd\",\n",
    "        lr=0.0005,\n",
    "        scheduler_patience=3\n",
    "    )\n",
    "\n",
    "    # Set a seed\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # Initialize the model prior to conv filter initialization\n",
    "    eu.models.init_weights(model)\n",
    "\n",
    "    # Return the model\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test out a model before training\n",
    "model = prep_new_model(0)\n",
    "print(model.summary())\n",
    "sdataloader = sdata_training_ST[:64].to_dataset().to_dataloader()\n",
    "test_seqs = next(iter(sdataloader))\n",
    "print(model(test_seqs[1], test_seqs[2]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a model on each target prediction!\n",
    "for i, target_col in enumerate(target_cols_ST):\n",
    "    print(f\"Training DeepBind SingleTask model on {target_col}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = prep_new_model(seed=i, conv_dropout=0.5, fc_dropout=0.5, batchnorm=True)\n",
    "\n",
    "    # Train the model\n",
    "    eu.train.fit(\n",
    "        model=model, \n",
    "        sdata=sdata_training_ST, \n",
    "        gpus=1, \n",
    "        target_keys=target_col,\n",
    "        train_key=\"train_val\",\n",
    "        epochs=5,\n",
    "        early_stopping_metric=\"val_loss\",\n",
    "        early_stopping_patience=3,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        name=\"DeepBind_ST\",\n",
    "        seed=i,\n",
    "        version=target_col,\n",
    "        verbosity=logging.ERROR\n",
    "    )\n",
    "    \n",
    "    # Get predictions on the training data\n",
    "    eu.evaluate.train_val_predictions(\n",
    "        model,\n",
    "        sdata=sdata_training_ST, \n",
    "        target_keys=target_col,\n",
    "        train_key=\"train_val\",\n",
    "        batch_size=1024,\n",
    "        num_workers=0,\n",
    "        name=\"DeepBind_ST\",\n",
    "        suffix=\"_ST\",\n",
    "        version=target_col\n",
    "    )\n",
    "    del model \n",
    "#sdata_training_ST.write_h5sd(os.path.join(eu.settings.output_dir, \"DeepBind_ST\", \"norm_training_predictions_ST.h5sd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the version for saving\n",
    "model_version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "conv_dropout = 0.25\n",
    "fc_dropout = 0.25\n",
    "batchnorm = True\n",
    "model = eu.models.DeepBind(\n",
    "    input_len=41, # Length of padded sequences\n",
    "    output_dim=len(target_cols_MT), # Number of multitask outputs\n",
    "    strand=\"ss\", # Strand information to include, only forward strand\n",
    "    task=\"regression\", # Task type, regression in this case\n",
    "    optimizer=\"adam\", # Optimizer to use\n",
    "    optimizer_kwargs={}, # Default optimizer kwargs\n",
    "    lr=0.0005, # Learning rate to start with\n",
    "    scheduler_patience=2, # Number of epochs to wait before reducing learning rate\n",
    "    conv_kwargs=dict(channels=[4, 1024], conv_kernels=[16], dropout_rates=conv_dropout, batchnorm=batchnorm), # Convolutional layer kwargs\n",
    "    fc_kwargs=dict(hidden_dims=[512], dropout_rate=fc_dropout, batchnorm=batchnorm) # Fully connected layer kwargs\n",
    ")\n",
    "model.summary(), model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "eu.train.fit(\n",
    "    model=model,\n",
    "    sdata=sdata_training_MT,\n",
    "    gpus=1,\n",
    "    target_keys=target_cols_MT,\n",
    "    train_key=\"train_val\",\n",
    "    epochs=100,\n",
    "    early_stopping_metric=\"val_loss\",\n",
    "    early_stopping_patience=5,\n",
    "    batch_size=1024,\n",
    "    num_workers=0,\n",
    "    name=\"DeepBind_MT\",\n",
    "    seed=42,\n",
    "    version=f\"v{model_version}\",\n",
    "    verbosity=logging.ERROR\n",
    ")\n",
    "\n",
    "# Get predictions on the training data\n",
    "eu.evaluate.train_val_predictions(\n",
    "    model,\n",
    "    sdata=sdata_training_MT, \n",
    "    target_keys=target_cols_MT,\n",
    "    train_key=\"train_val\",\n",
    "    batch_size=1024,\n",
    "    num_workers=0,\n",
    "    name=\"DeepBind_MT\",\n",
    "    suffix=\"_MT\",\n",
    "    version=f\"v{model_version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the predictions!\n",
    "sdata_training_MT.write_h5sd(os.path.join(eu.settings.output_dir, \"DeepBind_MT\", f\"norm_training_predictions_v{model_version}_MT.h5sd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double check we predicted on all the columns\n",
    "np.sum(sdata_training_MT.seqs_annot.columns.str.contains(\"RNCMPT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Move on to the next model version if training multiple\n",
    "model_version = model_version + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 eugene_dev",
   "language": "python",
   "name": "eugene_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
