{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray et al 2013 Training \n",
    "**Authorship:**\n",
    "Adam Klie (last updated: *06/08/2023*)\n",
    "***\n",
    "**Description:**\n",
    "Notebook to perform simple training of *single task* and *multitask* models on the Ray et al (2013) dataset.\n",
    "Also take a look at the `ray13_training_ST.py` script for usage. The script was run because all 244 models took several hours to train.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning\n",
    "\n",
    "# EUGENe imports and settings\n",
    "import eugene as eu\n",
    "from eugene import models, train, evaluate, settings\n",
    "from eugene.models import zoo\n",
    "settings.dataset_dir = \"/cellar/users/aklie/data/eugene/revision/ray13\"\n",
    "settings.output_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/output/revision/ray13\"\n",
    "settings.logging_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/logs/revision/ray13\"\n",
    "settings.config_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/configs/ray13\"\n",
    "\n",
    "# EUGENe packages\n",
    "import seqdata as sd\n",
    "\n",
    "# Print versions\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Eugene version: {eu.__version__}\")\n",
    "print(f\"SeqData version: {sd.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning version: {pytorch_lightning.__version__}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in the SetA training `SeqData`'s for single task and multi-task models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the training SetA processed data for single task and multitask models\n",
    "sdata_training_ST = sd.open_zarr(os.path.join(settings.dataset_dir, \"norm_setA_ST.zarr\"))\n",
    "sdata_training_MT = sd.open_zarr(os.path.join(settings.dataset_dir, \"norm_setA_MT.zarr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the prediction columns for single task and multitask\n",
    "ST_vars = pd.Index(sdata_training_ST.data_vars.keys())\n",
    "target_mask_ST = ST_vars.str.contains(\"RNCMPT\")\n",
    "target_cols_ST = ST_vars[target_mask_ST]\n",
    "MT_vars = pd.Index(sdata_training_MT.data_vars.keys())\n",
    "target_mask_MT = MT_vars.str.contains(\"RNCMPT\")\n",
    "target_cols_MT = MT_vars[target_mask_MT]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train single task models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiation function\n",
    "from pytorch_lightning import seed_everything\n",
    "def prep_new_model(\n",
    "    seed,\n",
    "    conv_dropout = 0,\n",
    "    dense_dropout = 0,\n",
    "    batchnorm = True\n",
    "):\n",
    "    # Set a seed\n",
    "    seed_everything(seed)\n",
    "\n",
    "    model = models.zoo.DeepBind(\n",
    "        input_len=41, # Length of padded sequences\n",
    "        output_dim=1, # Number of multitask outputs\n",
    "        conv_kwargs=dict(input_channels=4, conv_channels=[16], conv_kernels=[16], dropout_rates=conv_dropout, batchnorm=batchnorm),\n",
    "        dense_kwargs=dict(hidden_dims=[32], dropout_rates=dense_dropout, batchnorm=batchnorm),\n",
    "    )\n",
    "    \n",
    "    # Initialize the model prior to conv filter initialization\n",
    "    models.init_weights(model)\n",
    "\n",
    "    module = models.SequenceModule(\n",
    "        arch=model,\n",
    "        task=\"regression\",\n",
    "        loss_fxn=\"mse\",\n",
    "        optimizer=\"adam\",\n",
    "        optimizer_lr=0.0005,\n",
    "        scheduler_kwargs=dict(patience=2)\n",
    "    )\n",
    "\n",
    "    # Return the model\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the function to grab a model\n",
    "model = prep_new_model(seed=13, conv_dropout=0.5, dense_dropout=0.5, batchnorm=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train a model on each target prediction! NOTE: this is configured for testing purposes, see the ray13_training_ST.py script for the full training\n",
    "for i, target_col in enumerate(target_cols_ST[:1]):\n",
    "    print(f\"Training DeepBind SingleTask model on {target_col}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model = prep_new_model(seed=i, conv_dropout=0.5, dense_dropout=0.5, batchnorm=True)\n",
    "\n",
    "    # Fit the model\n",
    "    train.fit_sequence_module(\n",
    "        model,\n",
    "        sdata_training_ST,\n",
    "        seq_var=\"ohe_seq\",\n",
    "        target_vars=target_col,\n",
    "        in_memory=True,\n",
    "        train_var=\"train_val\",\n",
    "        epochs=5,\n",
    "        batch_size=100,\n",
    "        num_workers=4,\n",
    "        prefetch_factor=2,\n",
    "        drop_last=False,\n",
    "        early_stopping_patience=3,\n",
    "        name=\"DeepBind_ST\",\n",
    "        version=target_col,\n",
    "        transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32), \"target\": lambda x: torch.tensor(x, dtype=torch.float32)},\n",
    "        seed=i\n",
    "    )\n",
    "\n",
    "    evaluate.train_val_predictions_sequence_module(\n",
    "        model,\n",
    "        sdata=sdata_training_ST,\n",
    "        seq_var=\"ohe_seq\",\n",
    "        target_vars=target_col,\n",
    "        in_memory=True,\n",
    "        train_var=\"train_val\",\n",
    "        batch_size=1024,\n",
    "        num_workers=4,\n",
    "        prefetch_factor=2,\n",
    "        name=\"DeepBind_ST\",\n",
    "        version=target_col,\n",
    "        transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32), \"target\": lambda x: torch.tensor(x, dtype=torch.float32)},\n",
    "        suffix=\"_ST\"\n",
    "    )\n",
    "\n",
    "# Save the predictions!\n",
    "sd.to_zarr(sdata_training_ST, os.path.join(settings.output_dir, f\"norm_setA_predictions_ST.zarr\"), mode=\"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multi-task model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the version for saving\n",
    "model_version = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture to be trained\n",
    "arch = models.zoo.DeepBind(\n",
    "    input_len=41, # Length of padded sequences\n",
    "    output_dim=len(target_cols_MT), # Number of multitask outputs\n",
    "    conv_kwargs=dict(input_channels=4, conv_channels=[1024], conv_kernels=[16], dropout_rates=0.25, batchnorm=0.25),\n",
    "    dense_kwargs=dict(hidden_dims=[512], dropout_rates=0.25, batchnorm=True),\n",
    ")\n",
    "\n",
    "# Initialize the model prior to conv filter initialization\n",
    "models.init_weights(arch)\n",
    "\n",
    "# Wrap the model in a SequenceModule\n",
    "model = models.SequenceModule(\n",
    "    arch=arch,\n",
    "    task=\"regression\",\n",
    "    loss_fxn=\"mse\",\n",
    "    optimizer=\"adam\",\n",
    "    optimizer_lr=0.0005,\n",
    "    scheduler_kwargs=dict(patience=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "train.fit_sequence_module(\n",
    "    model,\n",
    "    sdata_training_MT,\n",
    "    seq_var=\"ohe_seq\",\n",
    "    target_vars=target_cols_MT,\n",
    "    in_memory=True,\n",
    "    train_var=\"train_val\",\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    drop_last=False,\n",
    "    early_stopping_patience=5,\n",
    "    name=\"DeepBind_MT\",\n",
    "    version=f\"v{model_version}\",\n",
    "    transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32), \"target\": lambda x: torch.tensor(x, dtype=torch.float32)},\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get training predictions\n",
    "evaluate.train_val_predictions_sequence_module(\n",
    "    model,\n",
    "    sdata=sdata_training_MT,\n",
    "    seq_var=\"ohe_seq\",\n",
    "    target_vars=target_cols_MT,\n",
    "    in_memory=True,\n",
    "    train_var=\"train_val\",\n",
    "    batch_size=1024,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    name=\"DeepBind_MT\",\n",
    "    version=f\"v{model_version}\",\n",
    "    transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32), \"target\": lambda x: torch.tensor(x, dtype=torch.float32)},\n",
    "    suffix=\"_MT\"\n",
    ")\n",
    "\n",
    "# Save the predictions!\n",
    "sd.to_zarr(sdata_training_MT, os.path.join(settings.output_dir, f\"norm_setA_predictions_v{model_version}_MT.zarr\"), mode=\"w\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check we predicted on all the columns\n",
    "for zarr in [f\"norm_setA_predictions_ST.zarr\", f\"norm_setA_predictions_v{model_version}_MT.zarr\"]:\n",
    "    sdata = sd.open_zarr(os.path.join(settings.output_dir, zarr))\n",
    "    keys = pd.Index(sdata.data_vars.keys())\n",
    "    print(zarr, sdata.dims[\"_sequence\"], len(sdata.data_vars))\n",
    "    print(np.sum(keys.str.contains(\"RNCMPT\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
