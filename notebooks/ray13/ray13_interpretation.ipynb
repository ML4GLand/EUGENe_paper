{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray et al 2013 Intepretation\n",
    "**Authorship:**\n",
    "Adam Klie, *09/03/2022*\n",
    "***\n",
    "**Description:**\n",
    "Notebook to interpret the trained models on the Ray et al (2013) dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eugene as eu\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import logomaker as lm\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eu.settings.dataset_dir = \"/cellar/users/aklie/data/eugene/ray13\"\n",
    "eu.settings.output_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/output/ray13\"\n",
    "eu.settings.logging_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/logs/ray13\"\n",
    "eu.settings.config_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/configs/ray13\"\n",
    "eu.settings.figure_dir = \"/cellar/users/aklie/projects/EUGENe/EUGENe_paper/figures/ray13\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the test `SeqData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in the test data with all the sequences\n",
    "sdata_test = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, \"norm_setB_processed_ST.h5sd\"))\n",
    "target_mask = sdata_test.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols = sdata_test.seqs_annot.columns[target_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MT training labels\n",
    "sdata_training = eu.dl.read_h5sd(os.path.join(eu.settings.dataset_dir, eu.settings.dataset_dir, \"norm_setA_sub_MT.h5sd\"))\n",
    "target_mask_MT = sdata_training.seqs_annot.columns.str.contains(\"RNCMPT\")\n",
    "target_cols_MT = sdata_training.seqs_annot.columns[target_mask_MT]\n",
    "del sdata_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the targets to make sure they are 244 and 233\n",
    "len(target_cols), len(target_cols_MT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in top 10 single task models\n",
    "top_ST_tasks = pd.read_csv(os.path.join(eu.settings.output_dir, \"top_10_ST_intensities.tsv\"), sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the top 10 multitask models\n",
    "top_MT_tasks = pd.read_csv(os.path.join(eu.settings.output_dir, \"top_10_MT_intensities.tsv\"), sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shared index\n",
    "shared_top_10 = sorted(top_ST_tasks.index.intersection(top_MT_tasks.index))\n",
    "shared_top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indeces of the shared cols in the MT target cols\n",
    "shared_top_10_MT_idx = np.where(target_cols_MT.isin(shared_top_10))[0]\n",
    "shared_top_10_MT_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single task models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get per nuceotide feature attibutions\n",
    "for i, target_col in enumerate(shared_top_10):\n",
    "    print(f\"Intepreting DeepBind SingleTask model on {target_col}\")\n",
    "    model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_ST\", target_col, \"checkpoints\", \"*\"))[0]\n",
    "    model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "    eu.interpret.feature_attribution_sdata(\n",
    "        model,\n",
    "        sdata_test,\n",
    "        method=\"InputXGradient\",\n",
    "        suffix=f\"_{target_col}_ST\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "version = 0\n",
    "model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_MT\", f\"v{version}\", \"checkpoints\", \"*\"))[0]\n",
    "model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "for i, target_col in zip(shared_top_10_MT_idx, shared_top_10):\n",
    "    print(f\"Intepreting version{version} DeepBind MultiTask model on {target_col}, which is the {i}th index of prediction\")\n",
    "    eu.interpret.feature_attribution_sdata(\n",
    "        model,\n",
    "        sdata_test,\n",
    "        method=\"InputXGradient\",\n",
    "        target=int(i),\n",
    "        suffix=f\"_{target_col}_MT\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot feature attributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, target_col in enumerate(shared_top_10):\n",
    "    print(f\"Plotting feature attribution scores for DeepBind models on {target_col}\")\n",
    "    top5_index = sdata_test[target_col].sort_values(ascending=False).index[:5]\n",
    "    eu.pl.multiseq_track(\n",
    "        sdata_test,\n",
    "        seq_ids=top5_index,\n",
    "        uns_keys=[f\"InputXGradient_imps_{target_col}_ST\", f\"InputXGradient_imps_{target_col}_MT\"],\n",
    "        alphabet=\"RNA\",\n",
    "        width=30,\n",
    "        height=6,\n",
    "        ylabels=[\"InputXGradient SingleTask\", \"InputXGradient MultiTask\"],\n",
    "        save=os.path.join(eu.settings.figure_dir, \"feature_attr\", f\"model_top5_feature_attr_{target_col}_STandMT.pdf\")\n",
    "    )\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the pfms for the 16 filters of each single task model\n",
    "for i, target_col in enumerate(shared_top_10):\n",
    "    print(f\"Generating pfms for single task DeepBind models on {target_col}\")\n",
    "    model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_ST\", target_col, \"checkpoints\", \"*\"))[0]\n",
    "    model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "    eu.interpret.generate_pfms_sdata(\n",
    "        model=model, \n",
    "        sdata=sdata_test,\n",
    "        method=\"Alipahani15\",\n",
    "        threshold=0.75,\n",
    "        batch_size=2048,\n",
    "        vocab=\"RNA\",\n",
    "        key_name=f\"pfms_{target_col}_ST\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab the pfms for all filters of the multitask model, this requires a lot of mem!\n",
    "version = 0\n",
    "model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_MT\", f\"v{version}\", \"checkpoints\", \"*\"))[0]\n",
    "model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "eu.interpret.generate_pfms_sdata(\n",
    "    model=model, \n",
    "    sdata=sdata_test,\n",
    "    threshold=0.75, \n",
    "    method=\"Alipahani15\",\n",
    "    batch_size=2048,\n",
    "    vocab=\"RNA\",\n",
    "    key_name=f\"pfms_MT\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot filter viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizations for all 16 filters for DeepBind SingleTask models\n",
    "for i, target_col in enumerate(shared_top_10):\n",
    "    print(f\"Plotting and saving filter visualizations for DeepBind models on {target_col}\")\n",
    "    eu.pl.multifilter_viz(\n",
    "        sdata_test,\n",
    "        filter_ids=range(0,16),\n",
    "        uns_key=f\"pfms_{target_col}_ST\",\n",
    "        titles=[f\"filter {i}\" for i in range(16)],\n",
    "        num_rows=4,\n",
    "        num_cols=4,\n",
    "        save=os.path.join(eu.settings.figure_dir, \"filter_viz\", f\"filters_viz_{target_col}_0.75_ST.pdf\")\n",
    "    )\n",
    "\n",
    "    # Save all the filter pfms from above as meme format for submission to TomTom\n",
    "    eu.dl.motif.filters_to_meme_sdata(\n",
    "        sdata_test,\n",
    "        uns_key=f\"pfms_{target_col}_ST\", \n",
    "        vocab=\"RNA\",\n",
    "        output_dir=os.path.join(eu.settings.output_dir),\n",
    "        file_name=f\"{target_col}_filters_0.75_ST.meme\"\n",
    "    )\n",
    "    #plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualizations for all filters of the multitask model\n",
    "for i in range(32):\n",
    "    start_filter = i*32\n",
    "    end_filter = (i*32) + 32\n",
    "    print(f\"Plotting and saving filters {start_filter+1}-{end_filter}\")\n",
    "    eu.pl.multifilter_viz(\n",
    "        sdata_test,\n",
    "        filter_ids=list(sdata_test.uns[\"pfms_MT\"].keys())[start_filter:end_filter],\n",
    "        num_rows=8,\n",
    "        num_cols=4,\n",
    "        uns_key=\"pfms_MT\",\n",
    "        titles=[f\"filter {i}\" for i in range(start_filter, end_filter)],\n",
    "        save=os.path.join(eu.settings.figure_dir, \"filter_viz\", f\"filters{start_filter+1}-{end_filter}_viz_MT.pdf\")\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the filter pfms from above as meme format for submission to TomTom\n",
    "eu.dl.motif.filters_to_meme_sdata(\n",
    "    sdata_test,\n",
    "    uns_key=f\"pfms_MT\", \n",
    "    vocab=\"RNA\",\n",
    "    output_dir=os.path.join(eu.settings.output_dir),\n",
    "    file_name=f\"filters_0.75_MT.meme\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In silico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ST model for an RBP of interest\n",
    "rbp = shared_top_10[4]\n",
    "model_file = glob.glob(os.path.join(eu.settings.logging_dir, \"DeepBind_ST\", \"checkpoints\", \"*\"))[0]\n",
    "model = eu.models.DeepBind.load_from_checkpoint(model_file)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve a set of 10 random sequences across 5 rounds\n",
    "random_seqs = eu.pp.ohe_seqs(eu.utils.random_seqs(10, 41))\n",
    "X_random = torch.Tensor(random_seqs)\n",
    "evolved_seqs = []\n",
    "mutation_pos = []\n",
    "for random_seq in random_seqs:\n",
    "    evolved_res = eu.interpret.evolution(\n",
    "        model=model,\n",
    "        X=random_seq,\n",
    "        rounds=5,\n",
    "    )\n",
    "    evolved_seqs.append(evolved_res[0])\n",
    "    mutation_pos.append(evolved_res[2])\n",
    "X_evolved = torch.Tensor(np.array(evolved_seqs))\n",
    "mutation_pos = np.array(mutation_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scores and the feature attributions for both the original random and the evolved\n",
    "random_scores = model(X_random)\n",
    "random_explains = eu.interpret.nn_explain(\n",
    "    model=model,\n",
    "    inputs=(X_random, X_random),\n",
    "    saliency_type=\"InputXGradient\",\n",
    ")\n",
    "evolved_scores = model(X_evolved)\n",
    "evolved_explains = eu.interpret.nn_explain(\n",
    "    model=model,\n",
    "    inputs=(X_evolved, X_evolved),\n",
    "    saliency_type=\"InputXGradient\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complicated plotting that we will eventually turn into a built in function\n",
    "for i in range(len(random_explains)):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10, 4))\n",
    "    random_viz_seq = pd.DataFrame(random_explains[i].T, columns=[\"A\", \"C\", \"G\", \"U\"])\n",
    "    random_viz_seq.index.name = \"pos\"\n",
    "    random_logo = lm.Logo(random_viz_seq, color_scheme=\"classic\", figsize=(10, 2), ax=ax[0])\n",
    "    random_logo.style_spines(visible=False)\n",
    "    random_logo.style_spines(spines=['left'], visible=True)\n",
    "    random_logo.ax.set_xticks([])\n",
    "    ax[0].vlines(mutation_pos[i]-0.5, 0, 1, transform=ax[0].get_xaxis_transform(), colors='r', linestyle='--')\n",
    "    ax[0].vlines(mutation_pos[i]+0.5, 0, 1, transform=ax[0].get_xaxis_transform(), colors='r', linestyle='--')\n",
    "    ax[0].set_title(f\"{random_scores[i].item():.2f} -> {evolved_scores[i].item():.2f}\")\n",
    "    ax_bottom = ax[0].get_ylim()[0]\n",
    "    for j in range(len(mutation_pos[i])):\n",
    "        ax[0].annotate(f\"{j+1}\", xy=(mutation_pos[i][j]-0.25, ax_bottom))\n",
    "    evolved_viz_seq = pd.DataFrame(evolved_explains[i].T, columns=[\"A\", \"C\", \"G\", \"U\"])\n",
    "    evolved_viz_seq.index.name = \"pos\"\n",
    "    evolved_logo = lm.Logo(evolved_viz_seq, color_scheme=\"classic\", figsize=(10, 2), ax=ax[1])\n",
    "    evolved_logo.style_spines(visible=False)\n",
    "    evolved_logo.style_spines(spines=['left'], visible=True)\n",
    "    evolved_logo.ax.set_xticks([])\n",
    "    ax[1].vlines(mutation_pos[i]-0.5, 0, 1, transform=ax[1].get_xaxis_transform(), colors='r', linestyle='--')\n",
    "    ax[1].vlines(mutation_pos[i]+0.5, 0, 1, transform=ax[1].get_xaxis_transform(), colors='r', linestyle='--')\n",
    "    plt.savefig(os.path.join(eu.settings.figure_dir, \"ise\", f\"randseq{i}_evolution_{rbp}.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the SeqData with predictions and interpretations\n",
    "sdata_test.write_h5sd(os.path.join(eu.settings.output_dir, \"norm_test_predictions_and_intepretations_MTfiltersonly_0.75.h5sd\"))\n",
    "#sdata_test.write_h5sd(os.path.join(eu.settings.output_dir, \"norm_test_predictions_and_intepretations_noMTfilters_0.75.h5sd\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f0aab14ae665ca4264878e5867720697752ca4d3a67458798aa51c276bf829a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
