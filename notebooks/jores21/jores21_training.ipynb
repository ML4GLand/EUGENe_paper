{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jores et al 2021 Training \n",
    "**Authorship:**\n",
    "Adam Klie, *05/18/2023*\n",
    "***\n",
    "**Description:**\n",
    "Notebook to perform simple training of models on the Jores et al (2021) dataset. You can also use the `jores21_training.py` script as well if you want to run it that way.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from copy import deepcopy \n",
    "\n",
    "# EUGENe imports and settings\n",
    "from eugene import models\n",
    "from eugene import train\n",
    "from eugene import settings\n",
    "settings.dataset_dir = \"/cellar/users/aklie/data/eugene/revision/jores21\"\n",
    "settings.output_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/output/revision/jores21\"\n",
    "settings.logging_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/logs/revision/jores21\"\n",
    "settings.config_dir = \"/cellar/users/aklie/projects/ML4GLand/EUGENe_paper/configs/jores21\"\n",
    "\n",
    "# EUGENe packages\n",
    "import seqdata as sd\n",
    "import motifdata as md"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in the `leaf`, `proto` and `combined` `SeqData`s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata_leaf = sd.open_zarr(os.path.join(settings.dataset_dir, \"jores21_leaf_train.zarr\"))\n",
    "sdata_proto = sd.open_zarr(os.path.join(settings.dataset_dir, \"jores21_proto_train.zarr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_seqdatas(seqdatas, keys):\n",
    "    for i, s in enumerate(seqdatas):\n",
    "        s[\"batch\"] = keys[i]\n",
    "    return xr.concat(seqdatas, dim=\"_sequence\")\n",
    "sdata_combined = concat_seqdatas([sdata_leaf, sdata_proto], [\"leaf\", \"proto\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in PFMs to initialize the 1st layer of the model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MotifSet with 78 motifs"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab motifs\n",
    "core_promoter_elements = md.read_meme(os.path.join(settings.dataset_dir, \"CPEs.meme\"))\n",
    "tf_clusters = md.read_meme(os.path.join(settings.dataset_dir, \"TF-clusters.meme\"))\n",
    "\n",
    "# Smush them together, make function in the future\n",
    "all_motifs = deepcopy(core_promoter_elements)\n",
    "for motif in tf_clusters:\n",
    "    all_motifs.add_motif(motif)\n",
    "all_motifs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for instantiating a new randomly initialized model\n",
    "def prep_new_model(\n",
    "    config,\n",
    "    seed\n",
    "):\n",
    "    # Instantiate the model\n",
    "    model = models.load_config(config_path=config, seed=seed)\n",
    "    \n",
    "    # Initialize the model prior to conv filter initialization\n",
    "    models.init_weights(model)\n",
    "\n",
    "    # Initialize the conv filters\n",
    "    if model.arch_name == \"Jores21CNN\":\n",
    "        layer_name = \"arch.biconv.kernels\"\n",
    "        list_index = 0\n",
    "    elif model.arch_name in [\"CNN\", \"Hybrid\", \"DeepSTARR\"]:\n",
    "        layer_name = \"arch.conv1d_tower.layers.0\"\n",
    "        list_index = None\n",
    "    models.init_motif_weights(\n",
    "        model=model,\n",
    "        layer_name=layer_name,\n",
    "        list_index=list_index,\n",
    "        motifs=all_motifs\n",
    "    )\n",
    "\n",
    "    # Return the model\n",
    "    return model \n",
    "\n",
    "# Instantiate test models to make sure this is working properly\n",
    "model = prep_new_model(\"cnn.yaml\", seed=0)\n",
    "model = prep_new_model(\"hybrid.yaml\", seed=0)\n",
    "model = prep_new_model(\"jores21_cnn.yaml\", seed=0)\n",
    "model = prep_new_model(\"deepstarr.yaml\", seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 5 models with 5 different random initializations\n",
    "training_sets = {\"leaf\": sdata_leaf, \"proto\": sdata_proto, \"combined\": sdata_combined}\n",
    "configs = [\"cnn.yaml\", \"hybrid.yaml\", \"jores21_cnn.yaml\", \"deepstarr.yaml\"]\n",
    "trials = 1\n",
    "for training_set in training_sets:\n",
    "    for trial in range(1, trials+1):\n",
    "        for config in configs:\n",
    "\n",
    "            # Print the model name\n",
    "            sdata = training_sets[training_set]\n",
    "            model_name = config.split(\".\")[0]\n",
    "            print(f\"{training_set} {model_name} trial {trial}\")\n",
    "\n",
    "            # Initialize the model\n",
    "            model = prep_new_model(config, seed=trial)\n",
    "\n",
    "            # Fit the model\n",
    "            train.fit_sequence_module(\n",
    "                model,\n",
    "                sdata,\n",
    "                seq_key=\"ohe_seq\",\n",
    "                target_keys=[\"enrichment\"],\n",
    "                in_memory=True,\n",
    "                train_key=\"train_val\",\n",
    "                epochs=1,\n",
    "                batch_size=128,\n",
    "                num_workers=4,\n",
    "                prefetch_factor=2,\n",
    "                drop_last=False,\n",
    "                name=model_name,\n",
    "                version=f\"{training_set}_trial_{trial}\",\n",
    "                seq_transforms={\"ohe_seq\": lambda x: torch.tensor(x, dtype=torch.float32).transpose(1, 2)},\n",
    "                seed=trial\n",
    "            )\n",
    "\n",
    "            # Make room for the next model \n",
    "            del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 ml4gland",
   "language": "python",
   "name": "ml4gland"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
